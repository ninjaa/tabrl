"""
Modal GPU training for custom reward functions generated by LLM
Based on the proven modal_playground_training.py approach
"""

import modal
import os
import json
from pathlib import Path
from datetime import datetime

# Create modal app
app = modal.App("tabrl-custom-reward-training")

# Create volume for model persistence
volume = modal.Volume.from_name("tabrl-models", create_if_missing=True)

# Create Modal image with dependencies - EXACT COPY from modal_playground_training.py
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install(["libegl1-mesa", "libopengl0", "libglu1-mesa", "git"])
    .pip_install(
        "pip>=24.1", # old pip can't do hyphens
        "jax[cuda12]==0.6.0", # 0.6.1 has a known cuSolver bug
        find_links="https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
    )
    .pip_install([
        "mujoco>=3.0.0",
        "mujoco_mjx>=3.0.0", 
        "brax>=0.11.0",
        "flax",
        "optax",
        "mediapy",
        "matplotlib",
        "numpy", 
        "orbax_checkpoint",
        "playground>=0.0.4",
    ])
)

@app.function(
    image=image,
    gpu="H100",
    volumes={"/workspace/models": volume},
    timeout=1800,
)
def train_custom_reward(
    scene_name: str,
    reward_function_code: str,
    reward_function_name: str,
    num_timesteps: int = 1_000_000,
    num_envs: int = 2048,
    batch_size: int = 1024,
    render_video: bool = False
):
    """Train a PPO agent with a custom reward function on a MuJoCo Playground scene."""
    
    import jax
    import jax.numpy as jnp
    import numpy as np
    from pathlib import Path
    from datetime import datetime
    import functools
    
    # Imports from mujoco_playground and brax
    from mujoco_playground import registry
    from mujoco_playground import wrapper
    from brax.training.agents.ppo import train as ppo
    from brax.training.agents.ppo import networks as ppo_networks
    
    import matplotlib.pyplot as plt
    import mediapy
    
    print(f"Training with custom reward: {reward_function_name}")
    print(f"Scene: {scene_name}")
    print(f"Timesteps: {num_timesteps:,}")
    
    # Compile the custom reward function
    print("Compiling custom reward function...")
    # Fix the reward function to remove float() conversion that breaks JAX
    reward_function_code_fixed = reward_function_code.replace("return float(total_reward)", "return total_reward")
    
    reward_namespace = {"jnp": jnp, "jax": jax}
    exec(reward_function_code_fixed, reward_namespace)
    reward_fn = reward_namespace[reward_function_name]
    
    # Create an adapter to convert MJX Data to Brax-like state for reward function
    class BraxStateAdapter:
        """Adapter to make MJX Data look like Brax PipelineState for reward functions"""
        def __init__(self, mjx_data):
            self._data = mjx_data
            # Create sub-objects that mimic Brax state structure
            self.x = type('obj', (object,), {
                'pos': mjx_data.xpos,  # Body positions
                'rot': mjx_data.xquat  # Body rotations (quaternions)
            })
            self.xd = type('obj', (object,), {
                'vel': mjx_data.cvel[:, :3] if hasattr(mjx_data, 'cvel') else jnp.zeros((mjx_data.xpos.shape[0], 3)),  # Linear velocities
                'ang': mjx_data.cvel[:, 3:] if hasattr(mjx_data, 'cvel') else jnp.zeros((mjx_data.xpos.shape[0], 3))   # Angular velocities
            })
            self.q = mjx_data.qpos   # Joint positions
            self.qd = mjx_data.qvel  # Joint velocities
            self.contact = mjx_data.contact if hasattr(mjx_data, 'contact') else None
    
    # Wrap the reward function to use the adapter
    def adapted_reward_fn(mjx_data, action):
        brax_state = BraxStateAdapter(mjx_data)
        return reward_fn(brax_state, action)
    
    # Load environment from registry
    # For now use Go1JoystickFlatTerrain as base
    env_name = "Go1JoystickFlatTerrain"
    
    # Create environment
    print(f"Loading environment: {env_name}")
    base_env = registry.locomotion.load(env_name)
    
    # Create a custom environment wrapper that overrides the reward function
    class CustomRewardEnv:
        def __init__(self, base_env, reward_fn):
            self._base = base_env
            self._custom_reward_fn = reward_fn
            self._reward_call_count = 0
            
        def step(self, state, action):
            # IMPORTANT: Call the base step method to preserve all original behavior
            # This includes physics simulation, observations, termination, etc.
            next_state = self._base.step(state, action)
            
            # Only replace the reward with our custom reward
            # Everything else (physics, observations, done flags) remains unchanged
            # next_state.data IS the pipeline state
            custom_reward = self._custom_reward_fn(next_state.data, action)
            
            # Log first few rewards for debugging
            self._reward_call_count += 1
            if self._reward_call_count <= 5:
                print(f"Step {self._reward_call_count}: base reward={next_state.reward}, custom reward={custom_reward}")
            
            # Only modify the reward field, preserving everything else
            return next_state.replace(reward=custom_reward)
            
        def __getattr__(self, name):
            # Delegate all other attributes to the base environment
            return getattr(self._base, name)
    
    # Create the custom environment
    env = CustomRewardEnv(base_env, adapted_reward_fn)
    
    print("Environment loaded with custom reward function")
    
    # Setup PPO training parameters
    ppo_params = {
        'num_timesteps': num_timesteps,
        'num_evals': 10,
        'reward_scaling': 1,
        'episode_length': 1000,
        'normalize_observations': True,
        'action_repeat': 1,
        'unroll_length': 5,
        'num_minibatches': 32,
        'num_updates_per_batch': 4,
        'discounting': 0.97,
        'learning_rate': 3e-4,
        'entropy_cost': 1e-2,
        'num_envs': num_envs,
        'batch_size': batch_size,
    }
    
    # Training progress tracking
    training_data = {
        'steps': [],
        'rewards': [],
        'times': []
    }
    start_time = datetime.now()
    
    def progress(num_steps, metrics):
        training_data['steps'].append(num_steps)
        training_data['rewards'].append(metrics['eval/episode_reward'])
        training_data['times'].append(datetime.now())
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"Step {num_steps:,}: "
              f"Reward: {metrics['eval/episode_reward']:.2f}, "
              f"Time: {elapsed:.1f}s")
    
    # Setup training function
    train_fn = functools.partial(
        ppo.train,
        num_timesteps=ppo_params['num_timesteps'],
        num_evals=ppo_params['num_evals'],
        reward_scaling=ppo_params['reward_scaling'],
        episode_length=ppo_params['episode_length'],
        normalize_observations=ppo_params['normalize_observations'],
        action_repeat=ppo_params['action_repeat'],
        unroll_length=ppo_params['unroll_length'],
        num_minibatches=ppo_params['num_minibatches'],
        num_updates_per_batch=ppo_params['num_updates_per_batch'],
        discounting=ppo_params['discounting'],
        learning_rate=ppo_params['learning_rate'],
        entropy_cost=ppo_params['entropy_cost'],
        num_envs=ppo_params['num_envs'],
        batch_size=ppo_params['batch_size'],
        network_factory=ppo_networks.make_ppo_networks,
        progress_fn=progress
    )
    
    # Train the model
    print("\nStarting PPO training on GPU...")
    
    make_inference_fn, params, metrics = train_fn(
        environment=env,
        eval_env=env,
        wrap_env_fn=wrapper.wrap_for_brax_training,
    )
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"\nTraining completed in {training_time:.1f} seconds!")
    
    # Get sample state for obs_shape (needed for inference)
    rng = jax.random.PRNGKey(0)
    sample_state = env.reset(rng)
    
    # Save the model
    output_name = f"{env_name}_{reward_function_name}"
    model_path = Path(f"/workspace/models/{output_name}.pkl")
    model_path.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"Saving model to {model_path}...")
    final_return = training_data['rewards'][-1] if training_data['rewards'] else 0
    
    # Get last few training rewards as "eval" rewards for compatibility
    eval_rewards = training_data['rewards'][-5:] if len(training_data['rewards']) >= 5 else training_data['rewards']
    
    # Match exact model_data structure from modal_playground_training.py
    model_data = {
        'params': params,
        'env_name': env_name,
        'training_steps': num_timesteps,
        'training_time': training_time,
        'avg_reward': final_return,
        'avg_length': 1000,  # Default episode length
        'training_data': training_data,
        'obs_shape': sample_state.obs['state'].shape,
        'action_size': env.action_size,
        'eval_rewards': eval_rewards,  # Required for get_model_info
        'reward_function_code': reward_function_code,  # Extra field for custom training
        'reward_function_name': reward_function_name,  # Extra field for custom training
    }
    
    with open(model_path, "wb") as f:
        import pickle
        pickle.dump(model_data, f)
    
    # Save training plot
    plot_path = Path(f"/workspace/models/{output_name}_training.png")
    plt.figure(figsize=(10, 6))
    plt.plot(training_data['steps'], training_data['rewards'])
    plt.xlabel('Timesteps')
    plt.ylabel('Evaluation Return')
    plt.title(f'Training Progress: {reward_function_name}')
    plt.grid(True)
    plt.savefig(plot_path)
    plt.close()
    
    # Optionally render a rollout video
    if render_video:
        print("\nRendering rollout video...")
        video_path = Path(f"/workspace/models/{output_name}_rollout.mp4")
        
        # Evaluate trained policy
        eval_env = env  # Use original env for visualization
        jit_reset = jax.jit(eval_env.reset)
        jit_step = jax.jit(eval_env.step)
        jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))
        
        # Run a rollout
        rng = jax.random.PRNGKey(42)
        state = jit_reset(rng)
        rollout_states = []
        
        for _ in range(1000):  # 1000 steps
            act_rng, rng = jax.random.split(rng)
            obs = state.obs
            action, _ = jit_inference_fn(obs, act_rng)
            state = jit_step(state, action)
            rollout_states.append(state)
            
            if state.done:
                break
        
        # Render the rollout
        from brax.io import html
        # Get the MuJoCo model from the base environment
        if hasattr(eval_env, '_base'):
            # Our custom wrapper
            base_env = eval_env._base
        else:
            base_env = eval_env
            
        # MuJoCo Playground environments use mjx_model
        if hasattr(base_env, 'mjx_model'):
            # Extract states for rendering
            states_for_render = [s.data for s in rollout_states]
            # Create HTML visualization - use mjx_model directly without .ptr
            html_str = html.render(base_env.mjx_model, states_for_render)
        else:
            print("Warning: Could not find model for rendering")
            html_str = "<p>Rendering not available</p>"
        
        # Convert HTML to video using brax's utilities
        # Note: Direct video export might need additional setup
        # For now, save the HTML
        html_path = Path(f"/workspace/models/{output_name}_rollout.html")
        with open(html_path, "w") as f:
            f.write(html_str)
        print(f"Rollout visualization saved to {html_path}")
    
    # Commit volume changes
    volume.commit()
    
    return {
        'model_path': str(model_path),
        'plot_path': str(plot_path),
        'video_path': str(video_path) if render_video else None,
        'metadata': {
            'scene_name': scene_name,
            'env_name': env_name,
            'reward_function_name': reward_function_name,
            'num_timesteps': num_timesteps,
            'final_return': final_return,
            'training_time': training_time,
        },
    }


# Convenience function to train from a policy JSON file
@app.function(
    image=image,
    gpu="H100",
    volumes={"/workspace/models": volume},
    timeout=1800,
)
def train_from_policy_json(
    policy_json_content: str,
    num_timesteps: int = 1_000_000,
    num_envs: int = 2048,
    batch_size: int = 1024,
    render_video: bool = False
):
    """Train from a policy JSON file content that contains reward functions."""
    
    import json
    
    # Parse the policy JSON content
    policy_data = json.loads(policy_json_content)
    
    scene_name = policy_data.get("scene", "Go1JoystickFlatTerrain")
    reward_functions = policy_data.get("reward_functions", [])
    
    # For now, use the first reward function (dense)
    if not reward_functions:
        raise ValueError("No reward functions found in policy JSON")
    
    # Select the dense reward by default
    selected_reward = None
    for rf in reward_functions:
        if rf["type"] == "dense":
            selected_reward = rf
            break
    
    if not selected_reward:
        selected_reward = reward_functions[0]  # Fallback to first
    
    reward_function_code = selected_reward["reward"]
    reward_function_name = "reward_fn"  # The function is always named reward_fn in the code
    
    print(f"Using reward function: {selected_reward['name']} ({selected_reward['type']})")
    
    return train_custom_reward.remote(
        scene_name=scene_name,
        reward_function_code=reward_function_code,
        reward_function_name=reward_function_name,
        num_timesteps=num_timesteps,
        num_envs=num_envs,
        batch_size=batch_size,
        render_video=render_video
    )


# CLI entrypoint
@app.local_entrypoint()
def main(
    policy_json: str,
    reward_index: int = 0,
    num_timesteps: int = 500000,
    render_video: bool = False
):
    """CLI entrypoint for training from a policy JSON file"""
    print(f"\nðŸš€ Starting TabRL Custom Reward Training")
    print(f"  Policy JSON: {policy_json}")
    print(f"  Reward Index: {reward_index}")
    print(f"  Timesteps: {num_timesteps:,}")
    print(f"  Render Video: {render_video}")
    
    # Read the policy JSON file locally
    with open(policy_json, 'r') as f:
        policy_content = f.read()
    
    # Train from policy JSON content
    result = train_from_policy_json.remote(
        policy_json_content=policy_content,
        num_timesteps=num_timesteps,
        render_video=render_video,
    )
    
    print("\nðŸŽ‰ Training Results:")
    print(f"  Model saved to: {result['model_path']}")
    print(f"  Final return: {result['metadata']['final_return']:.2f}")
    print(f"  Training time: {result['metadata']['training_time']:.1f}s")
    
    if render_video:
        print(f"  Video available at: {result['video_path']}")
