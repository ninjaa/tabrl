{
  "task": "Make the robot swing its hips side to side while walking forward like a sassy dance move",
  "scene": "Go1JoystickFlatTerrain",
  "policy_code": "import jax\nimport jax.numpy as jnp\nimport flax.linen as nn\nfrom typing import Sequence\n\nclass PolicyNetwork(nn.Module):\n    \"\"\"Policy network for sassy hip-swinging walking behavior\"\"\"\n    hidden_sizes: Sequence[int] = (256, 256, 128)\n    activation: str = 'tanh'\n    \n    @nn.compact\n    def __call__(self, observations):\n        # Extract key state information\n        x = observations\n        \n        # Add time-based features for rhythmic hip movement\n        # Assume we can derive time from joint positions or add explicit time input\n        time_feature = jnp.sin(jnp.sum(x[:12]) * 0.1)  # Use joint positions as proxy for time\n        hip_rhythm = jnp.array([jnp.sin(time_feature * 2.0), jnp.cos(time_feature * 2.0)])\n        \n        # Concatenate original observations with rhythmic features\n        x = jnp.concatenate([x, hip_rhythm])\n        \n        # Process through hidden layers\n        for hidden_size in self.hidden_sizes:\n            if self.activation == 'tanh':\n                x = nn.tanh(nn.Dense(hidden_size)(x))\n            elif self.activation == 'relu':\n                x = nn.relu(nn.Dense(hidden_size)(x))\n            else:\n                x = nn.swish(nn.Dense(hidden_size)(x))\n        \n        # Output layer - assuming 12 actuators for quadruped\n        actions = nn.Dense(12)(x)\n        \n        # Apply tanh activation to bound actions\n        actions = nn.tanh(actions)\n        \n        # Add bias towards hip swaying motion\n        # Assume hip joints are at indices 2, 5, 8, 11 (typical for quadruped)\n        hip_indices = jnp.array([2, 5, 8, 11])\n        hip_sway = 0.3 * jnp.sin(time_feature * 3.0)  # Side-to-side hip motion\n        \n        # Apply hip sway to relevant joints\n        actions = actions.at[hip_indices].add(hip_sway)\n        \n        # Ensure forward locomotion bias\n        forward_bias = 0.2 * jnp.ones_like(actions)\n        actions = actions + forward_bias\n        \n        return jnp.clip(actions, -1.0, 1.0)",
  "reward_functions": [
    {
      "name": "sassy_walk_dense",
      "type": "dense",
      "reward": "def reward_fn(state, action):\n    # Dense reward for sassy hip-swinging walk\n    \n    # Forward progress reward\n    forward_velocity = state.xd.vel[0, 0]  # x-velocity of root body\n    forward_reward = jnp.clip(forward_velocity, 0.0, 2.0) * 2.0\n    \n    # Hip swaying reward - lateral movement of torso\n    lateral_velocity = jnp.abs(state.xd.vel[0, 1])  # y-velocity magnitude\n    hip_sway_reward = jnp.clip(lateral_velocity, 0.0, 0.5) * 3.0\n    \n    # Rhythmic hip motion - reward periodic lateral movement\n    time_proxy = jnp.sum(state.q[:4]) * 0.1  # Use joint positions as time proxy\n    target_sway = 0.3 * jnp.sin(time_proxy * 2.0)\n    actual_sway = state.x.pos[0, 1]  # y-position of root\n    rhythm_reward = 1.0 - jnp.abs(target_sway - actual_sway)\n    \n    # Upright stability bonus\n    height = state.x.pos[0, 2]  # z-position (height)\n    upright_reward = jnp.clip(height, 0.2, 0.8) * 1.5\n    \n    # Energy efficiency penalty\n    energy_penalty = -0.001 * jnp.sum(action**2)\n    \n    # Combine rewards\n    total_reward = forward_reward + hip_sway_reward + rhythm_reward + upright_reward + energy_penalty\n    \n    return float(total_reward)"
    },
    {
      "name": "sassy_walk_sparse",
      "type": "sparse",
      "reward": "def reward_fn(state, action):\n    # Sparse reward for achieving sassy walking milestones\n    \n    # Check if robot is moving forward\n    forward_velocity = state.xd.vel[0, 0]\n    is_walking = forward_velocity > 0.5\n    \n    # Check if robot is swaying hips (lateral movement)\n    lateral_velocity = jnp.abs(state.xd.vel[0, 1])\n    is_swaying = lateral_velocity > 0.1\n    \n    # Check if robot maintains upright posture\n    height = state.x.pos[0, 2]\n    is_upright = height > 0.3\n    \n    # Check for rhythmic motion (change in lateral position)\n    current_y = state.x.pos[0, 1]\n    is_rhythmic = jnp.abs(current_y) > 0.05  # Some lateral displacement\n    \n    # Sparse rewards for achieving combinations of behaviors\n    reward = 0.0\n    \n    if is_walking and is_upright:\n        reward += 5.0  # Basic walking bonus\n    \n    if is_walking and is_swaying and is_upright:\n        reward += 10.0  # Sassy walking bonus\n    \n    if is_walking and is_swaying and is_rhythmic and is_upright:\n        reward += 20.0  # Full sassy dance walk bonus\n    \n    # Penalty for falling\n    if height < 0.2:\n        reward -= 10.0\n    \n    return float(reward)"
    },
    {
      "name": "sassy_walk_shaped",
      "type": "shaped",
      "reward": "def reward_fn(state, action):\n    # Shaped reward with progressive difficulty for sassy walking\n    \n    # Base forward locomotion (easy to achieve)\n    forward_velocity = state.xd.vel[0, 0]\n    forward_reward = jnp.tanh(forward_velocity) * 2.0\n    \n    # Progressive hip swaying difficulty\n    lateral_velocity = state.xd.vel[0, 1]  # Signed lateral velocity\n    lateral_position = state.x.pos[0, 1]   # Signed lateral position\n    \n    # Reward alternating lateral movement (shaped for learning)\n    time_proxy = jnp.sum(jnp.abs(state.qd[:6])) * 0.01  # Use joint velocities as time\n    target_lateral = 0.2 * jnp.sin(time_proxy * 4.0)  # Target sway pattern\n    \n    # Shaped reward for following sway pattern\n    sway_error = jnp.abs(lateral_position - target_lateral)\n    sway_reward = jnp.exp(-5.0 * sway_error) * 3.0  # Exponential shaping\n    \n    # Reward for lateral velocity matching target derivative\n    target_lateral_vel = 0.8 * jnp.cos(time_proxy * 4.0)\n    vel_error = jnp.abs(lateral_velocity - target_lateral_vel)\n    vel_reward = jnp.exp(-3.0 * vel_error) * 2.0\n    \n    # Stability shaping - gradually increase difficulty\n    height = state.x.pos[0, 2]\n    stability_reward = jnp.where(\n        height > 0.4,\n        2.0 * (height - 0.4),  # Bonus for extra height\n        5.0 * height  # Strong shaping when low\n    )\n    \n    # Smooth energy penalty\n    energy_penalty = -0.0005 * jnp.sum(jnp.tanh(action)**2)\n    \n    # Attitude bonus - reward for 'confident' movement\n    total_velocity = jnp.linalg.norm(state.xd.vel[0, :2])  # Combined x,y velocity\n    attitude_bonus = jnp.tanh(total_velocity * 2.0) * 1.0\n    \n    total_reward = forward_reward + sway_reward + vel_reward + stability_reward + energy_penalty + attitude_bonus\n    \n    return float(total_reward)"
    }
  ],
  "observation_space": 0,
  "action_space": 0
}
